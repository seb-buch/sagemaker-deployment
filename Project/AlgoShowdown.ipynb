{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of several algorithm for sentiment analysis\n",
    "\n",
    "This notebook is highly inspired and partially copid from https://towardsdatascience.com/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T13:44:54.405452Z",
     "start_time": "2020-04-13T13:44:53.793631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 125\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading and preprocessin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T13:44:57.007371Z",
     "start_time": "2020-04-13T13:44:54.407438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘../data/aclImdb_v1.tar.gz’ already there; not retrieving.\n",
      "../data/algoshowdown/test.csv already exist, nothin to do!\n",
      "../data/algoshowdown/train.csv already exist, nothin to do!\n"
     ]
    }
   ],
   "source": [
    "# Load data from web and prepare it to create the csv\n",
    "%mkdir -p ../data\n",
    "!wget -nc -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar --skip-old-files -zxf ../data/aclImdb_v1.tar.gz -C ../data\n",
    "\n",
    "data_dir = \"../data/algoshowdown\"\n",
    "imdb_rawdata_dir = \"../data/aclImdb\"\n",
    "%mkdir -p ../data/algoshowdown\n",
    "\n",
    "def prepare_imdb_data(dataset_type):\n",
    "    if dataset_type not in [\"train\", \"test\"]:\n",
    "        raise ValueError(\"dataset_type can only be 'train' or 'test'\")\n",
    "    \n",
    "    output_path = \"{}/{}.csv\".format(data_dir, dataset_type)\n",
    "    \n",
    "    if os.path.isfile(output_path):\n",
    "        print(\"{} already exist, nothin to do!\".format(output_path))\n",
    "        return\n",
    "    \n",
    "    input_path = \"{}/{}\".format(imdb_rawdata_dir, dataset_type)\n",
    "    \n",
    "    data_rows = []\n",
    "    for sentiment in [\"pos\", \"neg\"]:\n",
    "        folder = \"{}/{}\".format(input_path, sentiment)\n",
    "        \n",
    "        if sentiment == \"neg\":\n",
    "            is_negative = 1\n",
    "        else:\n",
    "            is_negative = 0\n",
    "            \n",
    "        print(\"importing data from '{}'... \".format(folder), end=\"\")\n",
    "        counter = 0\n",
    "        \n",
    "        for filename in glob.glob(\"{}/*.txt\".format(folder)):\n",
    "            counter += 1\n",
    "            with open(filename, \"r\") as fp:\n",
    "                text = fp.read()\n",
    "                \n",
    "                data_rows.append({\"text\": text, \"negative\": is_negative})\n",
    "        \n",
    "        print(\"{} reviews read\".format(counter))\n",
    "    \n",
    "    df = pd.DataFrame(data_rows)\n",
    "    df.to_csv(output_path)\n",
    "    print(\"reviews from '{}' saved to '{}'\".format(input_path, output_path))\n",
    "            \n",
    "prepare_imdb_data(\"test\")\n",
    "prepare_imdb_data(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T14:04:04.730211Z",
     "start_time": "2020-04-13T14:03:55.489227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading data from created csv files\n",
    "df_train = pd.read_csv('{}/train.csv'.format(data_dir)).sample(frac=1.0)\n",
    "df_test = pd.read_csv('{}/test.csv'.format(data_dir)).sample(frac=1.0)\n",
    "\n",
    "# Preprocessing our data\n",
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "X_train = tfidf.fit_transform(df_train['text']).toarray()\n",
    "X_test = tfidf.transform(df_test['text']).toarray()\n",
    "y_train, y_test = df_train['negative'], df_test['negative']\n",
    "X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=random_state)\n",
    "\n",
    "# Setting up our results dataframe\n",
    "df_results = pd.DataFrame(columns=['accuracy', 'run_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T13:45:18.292816Z",
     "start_time": "2020-04-13T13:45:18.285382Z"
    }
   },
   "outputs": [],
   "source": [
    "# models should a set populated with tuples (classifier_instance, is_early_stopping_needed_bool).\n",
    "# ex: models.append((myinstance, False))\n",
    "models = set()\n",
    "\n",
    "def add_model_to_showdown(model, need_early_stopping=False):\n",
    "    for registered_model, es in models:\n",
    "        if type(registered_model) == type(model):\n",
    "            return\n",
    "    \n",
    "    models.add((model, need_early_stopping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:09.511805Z",
     "start_time": "2020-04-13T09:58:09.184457Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=12, random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, False) # No need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:38:11.597385Z",
     "start_time": "2020-04-13T09:38:11.592164Z"
    }
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:09.749763Z",
     "start_time": "2020-04-13T09:58:09.513157Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "claasifier = RandomForestClassifier(n_estimators=500,\n",
    "                                    max_features=0.06,\n",
    "                                    n_jobs=6,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, False) # No need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:38:11.597385Z",
     "start_time": "2020-04-13T09:38:11.592164Z"
    }
   },
   "source": [
    "### Adaptive Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:09.955296Z",
     "start_time": "2020-04-13T09:58:09.751852Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_estim = DecisionTreeClassifier(max_depth=1, max_features=0.06)\n",
    "\n",
    "classifier = AdaBoostClassifier(base_estimator=base_estim,\n",
    "                                n_estimators=500,\n",
    "                                learning_rate=0.5,\n",
    "                                random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, False) # No need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:38:11.597385Z",
     "start_time": "2020-04-13T09:38:11.592164Z"
    }
   },
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:10.158388Z",
     "start_time": "2020-04-13T09:58:09.960756Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=2000,\n",
    "                                 subsample=0.67,\n",
    "                                 max_features=0.06,\n",
    "                                 validation_fraction=0.1,\n",
    "                                 n_iter_no_change=15,\n",
    "                                 verbose=0,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, False) # No need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:38:11.597385Z",
     "start_time": "2020-04-13T09:38:11.592164Z"
    }
   },
   "source": [
    "### HistGradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:10.377149Z",
     "start_time": "2020-04-13T09:58:10.160728Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "classifier = HistGradientBoostingClassifier(max_iter=2000,\n",
    "                                            validation_fraction=0.1,\n",
    "                                            n_iter_no_change=15,\n",
    "                                            verbose=0,\n",
    "                                            random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, False) # No need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:10.571075Z",
     "start_time": "2020-04-13T09:58:10.379206Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:58:10.786099Z",
     "start_time": "2020-04-13T09:58:10.574020Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(n_estimators=2000,\n",
    "                           tree_method='gpu_hist',\n",
    "                           subsample=0.67,\n",
    "                           colsample_level=0.06,\n",
    "                           verbose=0,\n",
    "                           n_jobs=6,\n",
    "                           random_state=random_state)\n",
    "\n",
    "add_model_to_showdown(classifier, True) # Need for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T09:46:42.637977Z",
     "start_time": "2020-04-13T09:46:42.634067Z"
    }
   },
   "source": [
    "### Long Short-Term Memory Classifier (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T16:43:40.083395Z",
     "start_time": "2020-04-13T16:43:39.999961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using cuda on GeForce RTX 2080 Ti\n",
      "Running epoch #1... torch.Size([50])\n",
      "torch.Size([1999, 50])\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-8ef829ac3f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m                \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                )\n",
      "\u001b[0;32m<ipython-input-99-8ef829ac3f50>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, epochs, eval_set, early_stopping_rounds, verbose)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-8ef829ac3f50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DataScience/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-8ef829ac3f50>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import sys\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.word_dict = None\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = torch.nn.BCELoss().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "        #self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        #    self.optimizer, 1, gamma=0.9)\n",
    "        self.batch_size = 50\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[lengths - 1, range(len(lengths))]\n",
    "        return self.sig(out.squeeze())\n",
    "\n",
    "    def train(self, X, y):\n",
    "        super(LSTMClassifier, self).train()\n",
    "        \n",
    "        # Convert X and y to tensors\n",
    "        X_tensor = torch.from_numpy(X).long().to(self.device)\n",
    "        y_tensor = torch.from_numpy(y.values).float().squeeze().to(self.device)\n",
    "\n",
    "        # Build the dataloader\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_tensor, y_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        for i, (text, label) in enumerate(loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self(text)\n",
    "            loss = self.criterion(output, label)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_acc += (output.round() == label).sum().item()\n",
    "\n",
    "        # Adjust the learning rate\n",
    "        #self.scheduler.step()\n",
    "\n",
    "        return train_loss / len(y), train_acc / len(y)\n",
    "\n",
    "    def test(self, X, y):\n",
    "        # Convert X and y to tensors\n",
    "        X_tensor = torch.from_numpy(X).long().to(self.device)\n",
    "        y_tensor = torch.from_numpy(y.values).float().squeeze().to(self.device)\n",
    "\n",
    "        # Build the dataloader\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_tensor, y_tensor),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "\n",
    "        for i, (text, label) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                output = self(text)\n",
    "                loss = self.criterion(output, label)\n",
    "                loss += loss.item()\n",
    "                acc += (output.round() == label).sum().item()\n",
    "\n",
    "        return loss / len(y), acc / len(y)\n",
    "\n",
    "    def fit(self, X_train,\n",
    "            y_train,\n",
    "            epochs=20,\n",
    "            eval_set=None,\n",
    "            early_stopping_rounds=3,\n",
    "            verbose=0,\n",
    "            ):\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training model using {}\".format(self.device), end=\"\")\n",
    "            if str(device) == \"cuda\":\n",
    "                print(\" on {}\".format(torch.cuda.get_device_name(\n",
    "                    torch.cuda.current_device())))\n",
    "            else:\n",
    "                print(\" (no info)\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        last_valid_acc = 0\n",
    "        n_bad_epochs = 0\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Running epoch #{}... \".format(epoch), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            start_time = time()\n",
    "            train_loss, train_acc = self.train(X_train, y_train)\n",
    "\n",
    "            if eval_set is not None:\n",
    "                valid_loss, valid_acc = self.test(\n",
    "                    eval_set[0][0], eval_set[0][1])\n",
    "\n",
    "            secs = int(time() - start_time)\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\" Done in {secs}s, Loss: {train_loss:.4f}(train) | Acc: {train_acc * 100:.1f}%(train)\", \n",
    "                    end=\"\")\n",
    "                if eval_set is None:\n",
    "                    print(\"\")\n",
    "                else:\n",
    "                    print(\n",
    "                    f\" , Loss: {valid_loss:.4f}(valid) | Acc: {valid_acc * 100:.1f}%(valid)\")\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "            if eval_set is not None:\n",
    "                if valid_acc - last_valid_acc < 0:\n",
    "                    n_bad_epochs += 1\n",
    "                if n_bad_epochs > early_stopping_rounds:\n",
    "                    print(\"Stopping training to avoid overfitting...\")\n",
    "                    break\n",
    "\n",
    "                last_valid_acc = valid_acc\n",
    "\n",
    "\n",
    "classifier = LSTMClassifier(32, 64, len(tfidf.vocabulary_)).to(device)\n",
    "\n",
    "classifier.fit(X_train_sub[:100],\n",
    "               y_train_sub[:100],\n",
    "               eval_set=[(X_valid[:100], y_valid[:100])],\n",
    "               early_stopping_rounds=5,\n",
    "               verbose=1,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T15:16:07.215199Z",
     "start_time": "2020-04-13T15:16:07.208541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T10:06:43.359264Z",
     "start_time": "2020-04-13T10:00:04.029612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBClassifier... Done in 41.6s\n",
      "Training HistGradientBoostingClassifier... Done in 61.3s\n",
      "Training DecisionTreeClassifier... Done in 13.8s\n",
      "Training AdaBoostClassifier... Done in 1.29e+02s\n",
      "Training GradientBoostingClassifier... Done in 1.07e+02s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model, es_needed in models:\n",
    "    name = model.__class__.__name__\n",
    "    \n",
    "    print(\"Training {}... \".format(name), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    start_time = time()\n",
    "    if es_needed:\n",
    "        model.fit(X_train_sub,\n",
    "              y_train_sub,\n",
    "              eval_set = [(X_valid, y_valid)],\n",
    "              early_stopping_rounds=5,\n",
    "              verbose=0)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    run_time = time() - start_time\n",
    "    accuracy = np.mean(model.predict(X_test) == y_test)\n",
    "        \n",
    "    df_results.loc[name] = [accuracy, run_time]\n",
    "    \n",
    "    print(\"Done in {:.3}s\".format(run_time))\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T13:28:05.524784Z",
     "start_time": "2020-04-13T13:28:05.448232Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9b67e8a32aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.bar(range(len(df_results)), df_results[\"accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
